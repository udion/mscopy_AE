{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Data stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.rand(1, 3, 101, 101)\n",
    "test_data = torch.rand(1, 3, 101, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nn.Conv2d(3, 64, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 256, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= n(Variable(train_data[0, :, :, :].view(1, 3, 512, 512)))\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Encoder (E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resBlock(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=64, k=3, s=1, p=1):\n",
    "        super(resBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, k, stride=s, padding=p)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, k, stride=s, padding=p)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        return self.bn2(self.conv2(y)) + x\n",
    "    \n",
    "class resTransposeBlock(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=64, k=3, s=1, p=1):\n",
    "        super(resTransposeBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, out_channels, k, stride=s, padding=p)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.ConvTranspose2d(out_channels, out_channels, k, stride=s, padding=p)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        return self.bn2(self.conv2(y)) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_res_blocks=5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_res_blocks = n_res_blocks\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1)\n",
    "        for i in range(n_res_blocks):\n",
    "            self.add_module('residual_block_1' + str(i+1), resBlock(in_channels=64, out_channels=64, k=3, s=1, p=1))\n",
    "        self.conv2 = nn.Conv2d(64, 32, 3, stride=2, padding=1)\n",
    "        for i in range(n_res_blocks):\n",
    "            self.add_module('residual_block_2' + str(i+1), resBlock(in_channels=32, out_channels=32, k=3, s=1, p=1))\n",
    "        self.conv3 = nn.Conv2d(32, 8, 3, stride=1, padding=1)\n",
    "        for i in range(n_res_blocks):\n",
    "            self.add_module('residual_block_3' + str(i+1), resBlock(in_channels=8, out_channels=8, k=3, s=1, p=1))\n",
    "        self.conv4 = nn.Conv2d(8, 1, 3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        for i in range(self.n_res_blocks):\n",
    "            y = self.__getattr__('residual_block_1'+str(i+1))(y)\n",
    "        y = self.conv2(y)\n",
    "        for i in range(self.n_res_blocks):\n",
    "            y = self.__getattr__('residual_block_2'+str(i+1))(y)\n",
    "        y = self.conv3(y)\n",
    "        for i in range(self.n_res_blocks):\n",
    "            y = self.__getattr__('residual_block_3'+str(i+1))(y)\n",
    "        y = self.conv4(y)\n",
    "        return y\n",
    "\n",
    "E1 = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = E1(Variable(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Decoder (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_res_blocks=5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_res_blocks = n_res_blocks\n",
    "        self.conv1 = nn.ConvTranspose2d(1, 8, 3, stride=1, padding=1)\n",
    "        for i in range(n_res_blocks):\n",
    "            self.add_module('residual_block_1' + str(i+1), resTransposeBlock(in_channels=8, out_channels=8, k=3, s=1, p=1))\n",
    "        self.conv2 = nn.ConvTranspose2d(8, 32, 3, stride=1, padding=1)\n",
    "        for i in range(n_res_blocks):\n",
    "            self.add_module('residual_block_2' + str(i+1), resTransposeBlock(in_channels=32, out_channels=32, k=3, s=1, p=1))\n",
    "        self.conv3 = nn.ConvTranspose2d(32, 64, 3, stride=2, padding=1)\n",
    "        for i in range(n_res_blocks):\n",
    "            self.add_module('residual_block_3' + str(i+1), resTransposeBlock(in_channels=64, out_channels=64, k=3, s=1, p=1))\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        for i in range(self.n_res_blocks):\n",
    "            y = self.__getattr__('residual_block_1'+str(i+1))(y)\n",
    "        y = self.conv2(y)\n",
    "        for i in range(self.n_res_blocks):\n",
    "            y = self.__getattr__('residual_block_2'+str(i+1))(y)\n",
    "        y = self.conv3(y)\n",
    "        for i in range(self.n_res_blocks):\n",
    "            y = self.__getattr__('residual_block_3'+str(i+1))(y)\n",
    "        y = self.conv4(y)\n",
    "        return y\n",
    "\n",
    "D1 = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 26, 26])\n",
      "torch.Size([1, 3, 101, 101])\n"
     ]
    }
   ],
   "source": [
    "print(t1.size())\n",
    "a = D1(t1)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it in box, VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, batchsz):\n",
    "        super(VAE, self).__init__()\n",
    "        self.E = encoder\n",
    "        self.D = decoder\n",
    "        self.batchsz = batchsz\n",
    "        self._enc_mu = nn.Linear(26*26, 128)\n",
    "        self._enc_log_sigma = nn.Linear(26*26, 128)\n",
    "        self._din_layer = nn.Linear(128, 26*26)\n",
    "        \n",
    "    def _sample_latent(self, h_enc):\n",
    "        '''\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        '''\n",
    "        mu = self._enc_mu(h_enc)\n",
    "        log_sigma = self._enc_log_sigma(h_enc)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_sigma = sigma\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_enc = self.E(x)\n",
    "        h_enc = h_enc.view(self.batchsz, 1, -1)\n",
    "        z = self._sample_latent(h_enc)\n",
    "        z = self._din_layer(z)\n",
    "        z = z.view(self.batchsz, 1, 26, 26)\n",
    "        return self.D(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = VAE(E1, D1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       " -2.4041e-02  1.0406e-01  4.8794e-02  ...   7.3427e-02  1.2258e-01  5.3513e-02\n",
       " -3.9071e-01  2.2520e-01  2.1490e-01  ...  -9.0915e-02  4.6186e-01  8.8965e-02\n",
       " -1.9377e-01  7.4536e-02  3.5039e-02  ...   1.6370e-01  7.4748e-02  3.5968e-02\n",
       "                 ...                   ⋱                   ...                \n",
       "  8.3426e-02  3.9855e-01 -1.5048e-01  ...  -1.2913e-01  8.7150e-03  2.2838e-01\n",
       "  4.4554e-03 -1.0679e+00 -1.8006e-01  ...  -1.1145e-01 -6.5338e-01 -7.7800e-01\n",
       " -1.4854e-01 -5.5733e-02  7.4461e-02  ...   1.1889e-03  5.4561e-01  7.5355e-03\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " -1.3546e-01 -1.4763e-02 -3.8727e-01  ...  -2.6202e-01 -2.9772e-01 -1.2940e-01\n",
       "  1.3664e-01  3.8997e-01  2.2444e-01  ...   2.4836e-01  4.8468e-01  2.1742e-02\n",
       "  1.2602e-01  1.8277e-01  6.1524e-02  ...  -2.7510e-02  2.7225e-02 -1.5069e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.1608e-01 -6.1379e-02 -1.2789e-01  ...   1.2331e-01 -6.2389e-01 -2.8936e-01\n",
       "  5.3776e-02  4.7326e-01 -2.7170e-01  ...  -4.9047e-01  1.7174e-02 -2.4550e-02\n",
       "  1.1162e-01 -1.5164e-01 -1.8123e-01  ...  -3.4378e-01 -1.2598e-01 -1.7911e-01\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  1.3027e-01 -9.2547e-03  2.0149e-01  ...   1.0806e-01  1.9786e-01  1.5510e-01\n",
       "  1.7162e-01  1.6826e-01  1.9871e-01  ...  -1.8856e-02 -2.5926e-01  1.2934e-01\n",
       "  1.1774e-01  3.7600e-01  1.9873e-01  ...   2.6731e-01  4.7114e-01  1.7913e-01\n",
       "                 ...                   ⋱                   ...                \n",
       " -4.8900e-02  7.0586e-02 -5.1325e-02  ...   1.9271e-01 -3.4535e-02  2.5137e-01\n",
       "  2.4007e-03  4.9412e-01 -4.7070e-02  ...  -7.0068e-02  1.3288e-01  1.1614e-01\n",
       "  1.9734e-02  2.0860e-01  1.1348e-01  ...   7.0848e-02  2.0801e-01  1.9809e-01\n",
       "[torch.FloatTensor of size 1x3x101x101]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V(Variable(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training thingy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_loss(z_mean, z_stddev):\n",
    "    mean_sq = z_mean * z_mean\n",
    "    stddev_sq = z_stddev * z_stddev\n",
    "    return 0.5 * torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
